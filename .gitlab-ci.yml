image: dtzar/helm-kubectl:3.8.0

stages:
  - build
  - review
  - check
  - staging
  - canary
  - production
  - cleanup

build:
  stage: build
  image:
    name: gcr.io/kaniko-project/executor:debug
    entrypoint: [""]
  script:
    - echo "{\"auths\":{\"$CI_REGISTRY\":{\"username\":\"$CI_REGISTRY_USER\",\"password\":\"$CI_REGISTRY_PASSWORD\"}}}" > /kaniko/.docker/config.json
    - /kaniko/executor --context $CI_PROJECT_DIR --dockerfile $CI_PROJECT_DIR/Dockerfile --destination "$CI_APPLICATION_REPOSITORY:$CI_APPLICATION_TAG"
  only:
    - master
    - branches

review:
  stage: review
  script:
    - mkdir -p ~/.kube/
    - check_kube_domain
    - create_secret
    - deploy
    - >-
         curl
         -n
         -X POST $SLACK_URL
         --data-urlencode 'payload={"channel": "#blog", "text": "Review branch is availabe on '"$CI_ENVIRONMENT_URL"' for branch '"$CI_COMMIT_REF_NAME"'"}'
    - echo "$CI_ENVIRONMENT_URL" > env-url

  environment:
    name: review/$CI_COMMIT_REF_NAME
    url: https://$CI_PROJECT_PATH_SLUG-$CI_ENVIRONMENT_SLUG.$AUTO_DEVOPS_DOMAIN
    on_stop: stop_review
  only:
    refs:
      - branches
    kubernetes: active
  except:
    - master
  artifacts:
    paths:
      - env-url
    expire_in: 1 days

linkChecker:
  stage: check
  image:
      name: linkchecker/linkchecker
      entrypoint: [""]
  script:
      - export ENV_URL=$(cat env-url)
      - loopLinckChecker $ENV_URL
  only:
    refs:
      - branches
    kubernetes: active
  except:
    - master


stop_review:
  stage: cleanup
  variables:
    GIT_STRATEGY: none
  script:
    - mkdir -p ~/.kube/
    - install_dependencies
    - delete
  environment:
    name: review/$CI_COMMIT_REF_NAME
    action: stop
  when: manual
  allow_failure: true
  only:
    refs:
      - branches
    kubernetes: active
  except:
    - master

# This job continuously deploys to production on every push to `master`.
# To make this a manual process, either because you're enabling `staging`
# or `canary` deploys, or you simply want more control over when you deploy
# to production, uncomment the `when: manual` line in the `production` job.

production:
  stage: production
  script:
    - mkdir -p ~/.kube/
    - check_kube_domain
    - create_secret
    - deploy
    - delete canary
  environment:
    name: production-blog
    url: https://$CI_PROJECT_PATH_SLUG.$AUTO_DEVOPS_DOMAIN
#  when: manual
  only:
    refs:
      - master
    kubernetes: active

# ---------------------------------------------------------------------------

.auto_devops: &auto_devops |
  # Auto DevOps variables and functions
  [[ "$TRACE" ]] && set -x
  export CI_APPLICATION_REPOSITORY=$CI_REGISTRY_IMAGE/$CI_COMMIT_REF_SLUG
  export CI_APPLICATION_TAG=$CI_COMMIT_SHA
  export CI_CONTAINER_NAME=ci_job_build_${CI_JOB_ID}

  function deploy() {
    track="${1-stable}"
    name="$CI_ENVIRONMENT_SLUG"

    if [[ "$track" != "stable" ]]; then
      name="$name-$track"
    fi

    replicas="1"
    service_enabled="false"
    postgres_enabled="$POSTGRES_ENABLED"
    # canary uses stable db
    [[ "$track" == "canary" ]] && postgres_enabled="false"

    env_track=$( echo $track | tr -s  '[:lower:]'  '[:upper:]' )
    env_slug=$( echo ${CI_ENVIRONMENT_SLUG//-/_} | tr -s  '[:lower:]'  '[:upper:]' )

    if [[ "$track" == "stable" ]]; then
      # for stable track get number of replicas from `PRODUCTION_REPLICAS`
      eval new_replicas=\$${env_slug}_REPLICAS
      service_enabled="true"
    else
      # for all tracks get number of replicas from `CANARY_PRODUCTION_REPLICAS`
      eval new_replicas=\$${env_track}_${env_slug}_REPLICAS
    fi
    if [[ -n "$new_replicas" ]]; then
      replicas="$new_replicas"
    fi
    echo "shizzle deploy $KUBE_NAMESPACE"
    helm upgrade --debug --dry-run --install \
      --wait \
      --set service.enabled="$service_enabled" \
      --set releaseOverride="$CI_ENVIRONMENT_SLUG" \
      --set image.repository="$CI_APPLICATION_REPOSITORY" \
      --set image.tag="$CI_APPLICATION_TAG" \
      --set image.pullPolicy=IfNotPresent \
      --set ingress.host="$CI_ENVIRONMENT_URL" \
      --set nameOverride="$name" \
      --namespace="$KUBE_NAMESPACE" \
      --version="$CI_PIPELINE_ID-$CI_JOB_ID" \
      "$name" \
      helm/

    helm upgrade --install \
      --wait \
      --set service.enabled="$service_enabled" \
      --set releaseOverride="$CI_ENVIRONMENT_SLUG" \
      --set image.repository="$CI_APPLICATION_REPOSITORY" \
      --set image.tag="$CI_APPLICATION_TAG" \
      --set image.pullPolicy=IfNotPresent \
      --set ingress.host="$CI_ENVIRONMENT_URL" \
      --set nameOverride="$name" \
      --namespace="$KUBE_NAMESPACE" \
      --version="$CI_PIPELINE_ID-$CI_JOB_ID" \
      "$name" \
      helm/
  }

  function check_kube_domain() {
    if [ -z ${AUTO_DEVOPS_DOMAIN+x} ]; then
      echo "In order to deploy or use Review Apps, AUTO_DEVOPS_DOMAIN variable must be set"
      echo "You can do it in Auto DevOps project settings or defining a secret variable at group or project level"
      echo "You can also manually add it in .gitlab-ci.yml"
      false
    else
      true
    fi
  }

  function create_secret() {
    kubectl create secret -n "$KUBE_NAMESPACE" \
      docker-registry gitlab-registry \
      --docker-server="$CI_REGISTRY" \
      --docker-username="$CI_DEPLOY_USER" \
      --docker-password="$CI_DEPLOY_PASSWORD" \
      --docker-email="$GITLAB_USER_EMAIL" \
      -o yaml --dry-run | sed 's/dockercfg/dockerconfigjson/g' | kubectl replace -n "$KUBE_NAMESPACE" --force -f -
   }

  function delete() {
    track="${1-stable}"
    name="$CI_ENVIRONMENT_SLUG"

    if [[ "$track" != "stable" ]]; then
      name="$name-$track"
    fi

    helm delete --purge "$name" || true
  }


   function loopLinckChecker() {
      nrOfRetries=10
      urlToCheck=$1
      oneError="That's it. 1 link in 1 URL checked. 0 warnings found. 1 error found.";

      counter=0
      while [ $counter -lt $nrOfRetries ]
      do
        counter=`expr $counter + 1`
        echo "--------- Start Loop ... number $counter ------"

        doLinkCheck $urlToCheck

        countOneErrorMesssages=$(grep -c "$oneError" linkchecker-out.txt)||:
        count404ErrorMesssages=$(grep -c "$four404Error" linkchecker-out.txt)||:

        if [ $countOneErrorMesssages -eq 1 -a $count404ErrorMesssages -gt 0 ];
         then
           echo "The site is not up yet! Wait for it..."
           sleep 5s
           echo "Lets try again."
         else
           echo "Links were checked"
           echo "**********Linkchecker output******************"
           cat linkchecker-out.txt
           echo "***************************************"
           succes=$(grep -c "^That's it.*0 errors found\." linkchecker-out.txt)

           if [ $succes -eq 1 ];
           then
              exit 0
           else
             exit 1
           fi
         fi
        echo "-----------Loop $counter END----------"
      done
      echo "Site $urlToCheck not available after $nrOfRetries retries"
      exit 404
    }

    function doLinkCheck(){
        urlToCheck=$1
        # If running with set -o pipefail, a failure at any stage in a shell pipeline will cause the entire pipeline to be considered failed.
        #In order to avoid this we explicitly ignore a single failure: { linkchecker xx || :; }
        result=$( (linkchecker --config=linkcheckerrc/linkcheckerrc $urlToCheck) ||:)
        echo "*********Checker done******************"
    }

before_script:
  - *auto_devops
