image: alpine:latest

stages:
  - build
  - review
  - check
  - staging
  - canary
  - production
  - cleanup
  - sync
  
sync-to-github:
  image: ceregousa/ubuntu-git
  stage: sync
  script:
    - eval $(ssh-agent -s)
    - ssh-add <(echo "$GITHUB_SSH_KEY")
    - mkdir -p ~/.ssh
    - '[[ -f /.dockerenv ]] && echo -e "Host *\n\tStrictHostKeyChecking no\n\n" > ~/.ssh/config'
    - git branch -avv
    - git checkout $CI_COMMIT_REF_NAME
    - git branch --set-upstream-to=origin/$CI_COMMIT_REF_NAME $CI_COMMIT_REF_NAME
    - git pull --all
    - git remote rm origin
    - git remote add origin git@github.com:tripledio/tripledio.github.io.git
    - git push -f origin '*:*'
    - git push -f origin --all -u
    - git branch -avv

build:
  stage: build
  image: docker:git
  services:
  - docker:dind
  script:
    - build
  only:
    - branches

review:
  stage: review
  script:
    - mkdir -p ~/.kube/
    - install_dependencies
    - init_helm
    - ensure_namespace
    - install_tiller
    - create_secret
    - deploy
  environment:
    name: review/$CI_COMMIT_REF_NAME
    url: http://$CI_PROJECT_PATH_SLUG-$CI_ENVIRONMENT_SLUG.$AUTO_DEVOPS_DOMAIN
    on_stop: stop_review
  only:
    refs:
      - branches
    kubernetes: active
  except:
    - master

linkChecker:
  stage: check
  image:
      name: linkchecker/linkchecker
      entrypoint: [""]
  script:
      - loopLinckChecker http://$CI_PROJECT_PATH_SLUG-$CI_ENVIRONMENT_SLUG.$AUTO_DEVOPS_DOMAIN
   #This is merely an indicator for gitlab so the necessary variables are passed on to this job
  environment:
      name: review/$CI_COMMIT_REF_NAME
  only:
    refs:
      - branches
    kubernetes: active
  except:
    - master


stop_review:
  stage: cleanup
  variables:
    GIT_STRATEGY: none
  script:
    - mkdir -p ~/.kube/
    - install_dependencies
    - delete
  environment:
    name: review/$CI_COMMIT_REF_NAME
    action: stop
  when: manual
  allow_failure: true
  only:
    refs:
      - branches
    kubernetes: active
  except:
    - master

# This job continuously deploys to production on every push to `master`.
# To make this a manual process, either because you're enabling `staging`
# or `canary` deploys, or you simply want more control over when you deploy
# to production, uncomment the `when: manual` line in the `production` job.

production:
  stage: production
  script:
    - mkdir -p ~/.kube/
    - install_dependencies
    - init_helm
    - ensure_namespace
    - install_tiller
    - create_secret
    - deploy
  environment:
    name: production
    url: http://$CI_PROJECT_PATH_SLUG.$AUTO_DEVOPS_DOMAIN
#  when: manual
  only:
    refs:
      - master
    kubernetes: active

# ---------------------------------------------------------------------------

.auto_devops: &auto_devops |
  # Auto DevOps variables and functions
  [[ "$TRACE" ]] && set -x
  export CI_APPLICATION_REPOSITORY=$CI_REGISTRY_IMAGE/$CI_COMMIT_REF_SLUG
  export CI_APPLICATION_TAG=$CI_COMMIT_SHA
  export CI_CONTAINER_NAME=ci_job_build_${CI_JOB_ID}
  export TILLER_NAMESPACE=$KUBE_NAMESPACE

  function codeclimate() {
    cc_opts="--env CODECLIMATE_CODE="$PWD" \
             --volume "$PWD":/code \
             --volume /var/run/docker.sock:/var/run/docker.sock \
             --volume /tmp/cc:/tmp/cc"

    docker run ${cc_opts} codeclimate/codeclimate init
    docker run ${cc_opts} codeclimate/codeclimate analyze -f json > codeclimate.json
  }

  function deploy() {
    name="$CI_ENVIRONMENT_SLUG"

    replicas="1"

    helm upgrade --debug --dry-run --install \
      --wait \
      --set service.enabled="$service_enabled" \
      --set releaseOverride="$CI_ENVIRONMENT_SLUG" \
      --set image.repository="$CI_APPLICATION_REPOSITORY" \
      --set image.tag="$CI_APPLICATION_TAG" \
      --set image.pullPolicy=IfNotPresent \
      --set service.url="$CI_ENVIRONMENT_URL" \
      --set replicaCount="$replicas" \
      --namespace="$KUBE_NAMESPACE" \
      --version="$CI_PIPELINE_ID-$CI_JOB_ID" \
      "$name" \
      chart/

    helm upgrade --install \
      --wait \
      --set service.enabled="$service_enabled" \
      --set releaseOverride="$CI_ENVIRONMENT_SLUG" \
      --set image.repository="$CI_APPLICATION_REPOSITORY" \
      --set image.tag="$CI_APPLICATION_TAG" \
      --set image.pullPolicy=IfNotPresent \
      --set service.url="$CI_ENVIRONMENT_URL" \
      --set replicaCount="$replicas" \
      --namespace="$KUBE_NAMESPACE" \
      --version="$CI_PIPELINE_ID-$CI_JOB_ID" \
      "$name" \
      chart/
  }

  function install_dependencies() {
    apk add -U openssl curl tar gzip bash ca-certificates git
    wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://raw.githubusercontent.com/sgerrand/alpine-pkg-glibc/master/sgerrand.rsa.pub
    wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.23-r3/glibc-2.23-r3.apk
    apk add glibc-2.23-r3.apk
    rm glibc-2.23-r3.apk

    curl https://kubernetes-helm.storage.googleapis.com/helm-v2.6.1-linux-amd64.tar.gz | tar zx
    mv linux-amd64/helm /usr/bin/
    helm version --client

    curl -L -o /usr/bin/kubectl https://storage.googleapis.com/kubernetes-release/release/v1.9.0/bin/linux/amd64/kubectl
    chmod +x /usr/bin/kubectl
    kubectl version --client
  }


  function init_helm() {
    helm init --client-only
  }

  function ensure_namespace() {
    kubectl describe namespace "$KUBE_NAMESPACE" || kubectl create namespace "$KUBE_NAMESPACE"
  }

  function build() {
    echo "Building Dockerfile-based application..."
    docker build -t "$CI_APPLICATION_REPOSITORY:$CI_APPLICATION_TAG" .

    if [[ -n "$CI_REGISTRY_USER" ]]; then
      echo "Logging to GitLab Container Registry with CI credentials..."
      docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" "$CI_REGISTRY"
      echo ""
    fi

    echo "Pushing to GitLab Container Registry..."
    docker push "$CI_APPLICATION_REPOSITORY:$CI_APPLICATION_TAG"
    echo ""
  }

  function install_tiller() {
    echo "Checking Tiller..."
    helm init --upgrade
    kubectl rollout status -n "$TILLER_NAMESPACE" -w "deployment/tiller-deploy"
    if ! helm version --debug; then
      echo "Failed to init Tiller."
      return 1
    fi
    echo ""
  }

  function create_secret() {
    kubectl create secret -n "$KUBE_NAMESPACE" \
      docker-registry gitlab-registry \
      --docker-server="$CI_REGISTRY" \
      --docker-username="$CI_REGISTRY_USER" \
      --docker-password="$CI_REGISTRY_PASSWORD" \
      --docker-email="$GITLAB_USER_EMAIL" \
      -o yaml --dry-run | sed 's/dockercfg/dockerconfigjson/g' | kubectl replace -n "$KUBE_NAMESPACE" --force -f -
   }

  function delete() {
    helm delete "$CI_ENVIRONMENT_SLUG" || true
  }


   function loopLinckChecker() {
      nrOfRetries=10
      urlToCheck=$1
      oneError="That's it. 1 link in 1 URL checked. 0 warnings found. 1 error found.";

      counter=0
      while [ $counter -lt $nrOfRetries ]
      do
        counter=`expr $counter + 1`
        echo "--------- Start Loop ... number $counter ------"

        doLinkCheck $urlToCheck

        countOneErrorMesssages=$(grep -c "$oneError" linkchecker-out.txt)||:
        count404ErrorMesssages=$(grep -c "$four404Error" linkchecker-out.txt)||:

        if [ $countOneErrorMesssages -eq 1 -a $count404ErrorMesssages -gt 0 ];
         then
           echo "The site is not up yet! Wait for it..."
           sleep 5s
           echo "Lets try again."
         else
           echo "Links were checked"
           echo "**********Linkchecker output******************"
           cat linkchecker-out.txt
           echo "***************************************"
           succes=$(grep -c "^That's it.*0 errors found\." linkchecker-out.txt)

           if [ $succes -eq 1 ];
           then
              exit 0
           else
             exit 1
           fi
         fi
        echo "-----------Loop $counter END----------"
      done
      echo "Site $urlToCheck not available after $nrOfRetries retries"
      exit 404
    }

    function doLinkCheck(){
        urlToCheck=$1
        # If running with set -o pipefail, a failure at any stage in a shell pipeline will cause the entire pipeline to be considered failed.
        #In order to avoid this we explicitly ignore a single failure: { linkchecker xx || :; }
        result=$( (linkchecker --config=linkcheckerrc/linkcheckerrc $urlToCheck) ||:)
        echo "*********Checker done******************"
    }

before_script:
  - *auto_devops